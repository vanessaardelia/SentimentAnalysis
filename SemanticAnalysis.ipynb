{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SemanticAnalysis.ipynb","provenance":[],"toc_visible":true,"mount_file_id":"1-G9xg6cvX4nmfLec5TpFzPPrc2ZUcbxz","authorship_tag":"ABX9TyNkXKSPQQW7bOHvFi04P+FB"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3dvdJkSmEMtW","executionInfo":{"status":"ok","timestamp":1606661358009,"user_tz":-420,"elapsed":2135,"user":{"displayName":"WILSON PHILIPS","photoUrl":"","userId":"10872635731392676463"}},"outputId":"a0b9285e-e29e-4060-9bcb-d4081297c982"},"source":["import pandas as pd\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SmegjpUtncv0"},"source":["**Membuat Daftar Kata**"]},{"cell_type":"code","metadata":{"id":"O-cU7tTvnSLI"},"source":["word_list = {}\n","for i in range(0, len(df['clean_text'])):\n","    sentence = df['clean_text'][i]\n","    # Tokenizing\n","    word_token = word_tokenize(sentence)\n","    # Mengecek setiap token\n","    for token in word_token:\n","        # Token yang tidak ditemukan pada word_list bernilai 0\n","        if token not in word_list:\n","            word_list[token] = 0\n","        else:\n","            word_list[token] += 1\n","\n","print(word_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZF78tKQDpSjO"},"source":["len(word_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BEsaoIF-6V-l"},"source":["**Mengimpor Data Leksikon**"]},{"cell_type":"code","metadata":{"id":"AtEjIHWLPYTy","colab":{"base_uri":"https://localhost:8080/","height":343},"executionInfo":{"status":"ok","timestamp":1606663372520,"user_tz":-420,"elapsed":832,"user":{"displayName":"WILSON PHILIPS","photoUrl":"","userId":"10872635731392676463"}},"outputId":"285bb684-1b0a-4f1b-99eb-ddcd4ea0f56a"},"source":["path = \"/content/drive/MyDrive/Project Sistem Pakar/modified_full_lexicon.csv\"\n","\n","# Membaca data leksikon\n","dict = pd.read_csv(path)\n","\n","negation_words = ['bukan','tidak','ga','gk']\n","\n","# Menghapus data yang termasuk kata negasi\n","dict = dict.drop(dict[(dict['word'] == 'bukan') | (dict['word'] == 'tidak') | (dict['word'] == 'ga') | (dict['word'] == 'gk')].index, axis=0)\n","\n","# Menyetel ulang indeks data\n","dict = dict.reset_index(drop=True)\n","\n","dict.head(10)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word</th>\n","      <th>weight</th>\n","      <th>number_of_words</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>hai</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>merekam</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ekstensif</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>paripurna</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>detail</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>pernik</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>belas</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>welas</td>\n","      <td>4</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>kabung</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>rahayu</td>\n","      <td>4</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        word  weight  number_of_words\n","0        hai       3                1\n","1    merekam       2                1\n","2  ekstensif       3                1\n","3  paripurna       1                1\n","4     detail       2                1\n","5     pernik       3                1\n","6      belas       2                1\n","7      welas       4                1\n","8     kabung       1                1\n","9     rahayu       4                1"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"FIZWh7Y26Z72","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606668122473,"user_tz":-420,"elapsed":2388,"user":{"displayName":"WILSON PHILIPS","photoUrl":"","userId":"10872635731392676463"}},"outputId":"fdb06574-024a-418a-b995-3e10e684673d"},"source":["len(dict)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10248"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gmxCWidN4Zid","executionInfo":{"status":"ok","timestamp":1606668137928,"user_tz":-420,"elapsed":1707,"user":{"displayName":"WILSON PHILIPS","photoUrl":"","userId":"10872635731392676463"}},"outputId":"77761670-0c9c-4ad7-db75-3993aa429434"},"source":["dict['number_of_words'].value_counts()"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1    9536\n","2     686\n","3      24\n","4       2\n","Name: number_of_words, dtype: int64"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"nTVdT0Nrx9is"},"source":["**Menampilkan Daftar Kata yang Tidak Ditemukan pada Data Leksikon**"]},{"cell_type":"code","metadata":{"id":"0NygWBwd493h"},"source":["# Menginisialisasi stemmer\n","stemmer_factory = StemmerFactory()\n","stemmer = stemmer_factory.create_stemmer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AlSvYl6zfsJ-"},"source":["dict_list = dict.to_list()\n","not_included_words = []\n","for word in word_list.keys():\n","    # Kata yang tidak ditemukan pada dict_list\n","    if word not in dict_list:\n","        # Stemming\n","        kata_dasar = stemmer.stem(word)\n","        # Kata dasar yang tidak ditemukan pada dict_list ditambahkan pada not_included_words\n","        if kata_dasar not in dict_list:\n","            not_included_words.append(word)\n","\n","print(not_included_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xpQhFbD2tFQy"},"source":["len(not_included_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"op6HHd3itfkt"},"source":["# Kata yang tidak ditemukan dan muncul lebih dari 3 kali \n","not_included_list = {k: v for (k, v) in word_list.items() if ((k in not_included_words) & (v > 3))}\n","\n","# Mengurutkan not_included_list berdasarkan jumlah kemunculan terbanyak\n","sort_items = sorted(not_included_list.items(), key = lambda x: x[1], reverse = True)\n","\n","# Mengambil 20 nilai not_included_list pertama\n","sort_items = sort_items[0:20]\n","\n","# Menampilkan not_included_list yang telah diurutkan\n","for i in sort_items:\n","    print(i[0], i[1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9SrmVcwKyrXK"},"source":["**Menampilkan Daftar Kata pada Grafik**"]},{"cell_type":"code","metadata":{"id":"ewWoXPOjyPFF"},"source":["# Fungsi untuk memfilter kalimat\n","def filter_sentence(sentence, key_list):\n","    # Tokenizing\n","    word_token = word_tokenize(sentence)\n","    new_sentence = ''\n","    # Mengecek setiap word_token\n","    for token in word_token:\n","        # Token yang tidak ditemukan pada key_list\n","        if token not in key_list:\n","            new_sentence = new_sentence + token + ' '\n","    return new_sentence"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R2nT-c60zKAt"},"source":["clean_text = df['clean_text'].copy()\n","\n","# Memfilter kata yang di-plotting dari negation_words\n","word_plot = clean_text.apply(lambda x: filter_sentence(x, negation_words))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J9xKDb7szOvi"},"source":["wordcloud = WordCloud(width = 800, height = 800, background_color = 'black', max_words = 1000, min_font_size = 20).generate(str(word_plot))\n","\n","# Plot dengan WordCloud\n","fig = plt.figure(figsize = (8, 8), facecolor = None)\n","plt.imshow(wordcloud)\n","plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3yQDMs-K4AYS"},"source":["**Menghitung Nilai Sentimen**"]},{"cell_type":"code","metadata":{"id":"VetD6zRY3-zK"},"source":["# Fungsi untuk menghitung nilai sentimen\n","def calculate_sentiment(index, word_token, word, sentiment):\n","    # Kata yang terdapat negation_words dibelakangnya memiliki nilai sentimen yang negatif\n","    if (word_token[index-1] in negation_words):\n","        sentiment += -dict['weight'][dict_list.index(word)]\n","    # Menambahkan nilai sentimen\n","    else:\n","        sentiment += dict['weight'][dict_list.index(word)]\n","    \n","    return sentiment"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2lOAzjJp5LzH"},"source":["sentiment_list = []\n","\n","# Menghitung nilai sentimen setiap kalimat\n","for i in range(len(df)):\n","    sentiment = 0 \n","    word_token = word_tokenize(df['clean_text'][i])\n","    \n","    # Mengecek token pada setiap kalimat\n","    for token in word_token:\n","        index = word_token.index(token)\n","        # Menghitung sentimen pada token yang terdapat pada data leksikon\n","        if token in dict_list :\n","            sentiment = calculate_sentiment(index, word_token, token, sentiment)\n","        else:\n","            kata_dasar = stemmer.stem(token)\n","            # Mengecek kata dasar dari token jika tidak terdapat pada data leksikon\n","            if kata_dasar in dict_list:\n","                sentiment = calculate_sentiment(index, word_token, kata_dasar, sentiment)\n","            # Mengecek kombinasi kata dengan kata sebelumnya\n","            elif(len(word_token) > 1):\n","                # Kombinasi dengan satu kata sebelumnya\n","                if(index-1 > -1):\n","                    combination_one = word_token[index-1] + ' ' + token\n","                    if (combination_one in dict_list):\n","                        sentiment= calculate_sentiment(index, word_token, combination_one, sentiment)\n","                    # Kombinasi dengan dua kata sebelumnya\n","                    elif(index-2 > -1):\n","                        combination_two = word_token[index-2] + ' ' + combination_one\n","                        if combination_two in dict_list:\n","                            sentiment= calculate_sentiment(index, word_token, combination_two, sentiment)\n","    \n","    sentiment_list.append(sentiment)\n","\n","# Menampilkan seluruh nilai sentimen\n","print(sentiment_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H7uHzM9cG8jr"},"source":["# Menambahkan sentiment_list pada df\n","sentiment_array = np.array(sentiment_list).reshape(df.shape[0], 1)\n","sentiment_data = np.hstack((df, sentiment_array))\n","new_df = pd.DataFrame(sentiment_data, columns = [...])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mSwL0HV6G-Q_"},"source":["new_df.head(10)"],"execution_count":null,"outputs":[]}]}